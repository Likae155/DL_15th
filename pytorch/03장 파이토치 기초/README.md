# 03장 파이토치 기초

## 개요
PyTorch의 기본 개념과 핵심 기능을 학습합니다. 텐서 연산부터 신경망 구현까지 딥러닝의 기초를 다룹니다.

---

## 예제 목록

### 텐서 기초
| 예제 | 파일명 | 주요 내용 |
|-----|--------|----------|
| 3.01-3.06 | `예제 3.01~3.06 텐서.ipynb` | 텐서 생성, 변환, 기본 연산 |
| 3.07-3.09 | `예제 3.07~3.09 텐서 연산.ipynb` | 텐서 산술/행렬 연산, 브로드캐스팅 |
| 3.10-3.12 | `예제 3.10~3.12 텐서 형태 변환.ipynb` | reshape, transpose, squeeze/unsqueeze |
| 3.13-3.14 | `예제 3.13~3.14 텐서 결합.ipynb` | cat, stack을 이용한 텐서 결합 |
| 3.15-3.17 | `예제 3.15~3.17 텐서 인덱싱, 슬라이싱.ipynb` | 텐서 데이터 접근 방법 |

### GPU 가속
| 예제 | 파일명 | 주요 내용 |
|-----|--------|----------|
| 3.18-3.19 | `예제 3.18~3.19 GPU 연산.ipynb` | CUDA 텐서, GPU 활용 |

### 선형 회귀
| 예제 | 파일명 | 주요 내용 |
|-----|--------|----------|
| 3.20-3.24 | `예제 3.20~3.24 단순 선형 회귀.ipynb` | 경사하강법, 손실함수, 옵티마이저 |
| 3.25 | `예제 3.25 zero_grad(), cost.backward(), optimizer.step().ipynb` | 학습 3단계 동작 원리 |
| 3.26-3.28 | `예제 3.26~3.28 신경망 패키지 적용.ipynb` | nn.Linear, nn.MSELoss 사용법 |
| 3.29-3.33 | `예제 3.29~3.33 다중 선형 회귀.ipynb` | DataLoader, 미니배치 학습 |
| 3.34 | `예제 3.34 편향 제거.ipynb` | bias=False 옵션 |
| 3.35-3.43 | `예제 3.35~3.43 비선형 회귀.ipynb` | Custom Dataset/Model, 모델 저장 |

### 모델 저장 및 불러오기
| 예제 | 파일명 | 주요 내용 |
|-----|--------|----------|
| 3.44-3.47 | `예제 3.44~3.47 모델 불러오기.ipynb` | state_dict, 전체 모델 저장/불러오기 |
| 3.48-3.52 | `예제 3.48~3.52 체크포인트.ipynb` | 학습 중간 저장, 재개 |

### 분류 모델
| 예제 | 파일명 | 주요 내용 |
|-----|--------|----------|
| 3.53-3.57 | `예제 3.53~3.57 이진 분류.ipynb` | 시그모이드, BCE 손실함수 |
| 3.58-3.59 | `예제 3.58~3.59 퍼셉트론.ipynb` | 단층/다층 퍼셉트론, XOR 문제 |
| 3.60-3.64 | `예제 3.60~3.64 활성화 함수.ipynb` | Sigmoid, Tanh, ReLU 등 |

---

## 핵심 개념

### 텐서 (Tensor)
- PyTorch의 기본 데이터 구조
- NumPy ndarray와 유사하나 GPU 연산 지원
- `requires_grad=True`로 자동 미분 활성화

### 학습 3단계
```python
optimizer.zero_grad()  # 1. 기울기 초기화
loss.backward()        # 2. 역전파
optimizer.step()       # 3. 파라미터 업데이트
```

### nn.Module
- 모든 신경망의 기본 클래스
- `__init__`: 레이어 정의
- `forward`: 순전파 로직 구현

---

## 필요 라이브러리
```python
import torch
from torch import nn, optim
from torch.utils.data import Dataset, DataLoader, TensorDataset
```
